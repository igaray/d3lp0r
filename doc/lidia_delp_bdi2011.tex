\documentclass{llncs2e/llncs}

\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{bibentry}
\usepackage[numbers]{natbib}

\title{An Argumentative Approach for a BDI Agent} 
        
\author{I\~{n}aki Garay \and      % igarai@gmail.com
        Diego Marcovecchio \and   % diegomarcov@gmail.com
        Leonardo Molas \and       % leos.molas@gmail.com
        Emiliano Montenegro \and  % emm.montenegro@gmail.com
        Fernando Sisul \and       % fsisul@gmail.com
        Manuel Torres \and        % jmtorresluc@gmail.com
        Sebastián Gottifredi \and % sebastian.gottifredi@gmail.com
        Alejandro García \and     % ajg@cs.uns.edu.ar
        Diego Martínez \and       % dcm@cs.uns.edu.ar
        Guillermo Simari          % grs@cs.uns.edu.ar
        }

\institute{Universidad Nacional del Sur \\\email{\{igarai,diegomarcov,leos.molas,%
emm.montenegro,fsisul,jmtorresluc\}@gmail.com,\\\{sg,ajg,dcm,grs\}%
@cs.uns.edu.ar}}

\begin{document}

\maketitle

\begin{abstract}
    This report presents the design and results of the d3lp0r multi-agent system 
    developed by the LIDIA team for the Multi-Agent Programming Contest 2011 (MAPC).
    The d3lp0r agents use a BDI architecture extended with planning and 
    argumentation (via Defeasible Logic Programming) to model a cooperating team 
    operating in a dynamic and competitive environment.

    In particular, the main goal of this report is to describe the chosen 
    architecture, the communication scheme and the way argumentation was put to 
    use in the agent's reasoning process and the technical details thereof.
\end{abstract}

\section{Introduction}

    The \texttt{d3lp0r} system was developed in the context of the Multi-Agent
    Programming Contest 2011 (MAPC) \cite{BehrensAMAI2010b} hosted by the 
    Clausthal University of
    Technology \footnote{More information in \texttt{www.tu-clausthal.de}}.

    The LIDIA (Laboratorio de Investigación y Desarrollo en Inteligencia 
    Artificial, Artificial Intelligence Research and Development Laboratory) 
    research group was established in 1992 at the Universidad Nacional del Sur. 
    The d3lp0r team was formed incorporating six graduate students, two Ph.D. 
    students and three professors. The undergraduate students fully developed the 
    system, while the Ph.D. students and professors provided guidance. The group's 
    main motivation was to apply argumentation \cite{Prakken:1997}
    \cite{Rahwan:2009} \cite{Bench-Capon:2007}\ via defeasible logic programming 
    (DeLP \cite{Garcia:2004a}) in a BDI based agent \cite{Amgoud:2008}, in the context of a 
    multi-agent gaming situation, 
    and to test the integration of the different technologies used.

    Experience from a previous instance of the MAPC was shared with our teams by 
    members of the ARGONAUTS team from TU Dortmund. Although the initial 
    plan was to run tests against other agent teams prior to the competition, time 
    constraints acted against this.

%\section{Preliminaries}
%
%    The \texttt{d3lp0r} system was developed in the context of the Multi-Agent
%    Programming Contest 2011 (MAPC) hosted by the Clausthal University of
%    Technology (www.tu-clausthal.de). 

%    The simulation scenario is represented as an undirected graph, in which 
%    nodes are valid agent locations weighted by value, and edges are valid
%    transitions weighted by cost. 
%    Agent state includes energy, health, and strength parameters. Agent percepts
%    include visible nodes, edges, and other agents. 
%    Each agent is assigned a role in the simulation (Explorer, Saboteur,
%    Repairer, Sentinel and Inspector) which determines its valid actions and
%    initial maximum values for the agent state parameters. 
%    Actions in general have an energy cost, movement action costs depend on edge
%    weights, successful attack actions decrease enemy agent's health subject to
%    a comparison of their strength attributes. Certain actions may cause further
%    information to be included in subsequent percepts. 
%
%    A match consists of several simulations, and each simulation proceeds as
%    a series of steps. In each step, each agent is provided with a percept with
%    partial information about the current simulation state and is
%    queried for its action. 
%
%    The goal is to maximize the score function each step. A team is awarded
%    points according to the value of the nodes controlled by the team, and
%    in addition certain achievements. Agents may control more nodes than 
%    those they are positioned in by forming zones, groups of nodes under the
%    influence of a team determined by a graph coloring algorithm specified in
%    the scenario description. 
%
%    The final score for a team is the sum of points over all steps; at the end
%    of the match, the team with the most points is the winner.

\section{System Analysis and Design}

    Despite many man-hours dedicated to design in the early stages of
    the competition, the development team's lack of experience in multi-agent
    systems made necessary several changes and additions. 
    Nevertheless, our approach was more than satisfying, resulting to be
    modular, correct and in close correspondence with the literature.

    The solution follows a decentralised architecture in which agents run 
    completely decoupled in different processes while sharing nothing. Percepts 
    are communicated among agent members of the team via a broadcast mechanism 
    developed as part of the multi-agent system. This design was chosen for its 
    minimal complexity.

    Agents are completely autonomous meaning that decision-making takes place 
    individually at the agent level, with no intervention from human operators or 
    a central intelligence agency within the system, and that decisions made by an 
    agent are influenced solely by the current simulation state and the results of 
    previous steps.
    The agent architecture developed is based on the BDI model \cite{Rao:1991}, 
    and is explained in detail in further sections.

    No design methodology specific to multi-agent systems was used. However, 
    the implementation was conducted using a simplified XP (extreme programming) 
    methodology.

    Approximately 1500 man-hours were invested in the team development.

\section{Software Architecture}


%\begin{figure}
%\centering
% \includegraphics[width=\textwidth]{agent_architecture.pdf}
%   \caption{Agent architecture in a flow chart-like diagram. 
%   Dashed arrows represent process flow, solid lines represent data flow.}
%\label{fig:architecture}
%\end{figure}

    In Fig. \ref{fig:architecture}, we sketch the architecture of our agent, 
    i.e. its interactions with the servers (MASSim, and our Percept Server), 
    and its internal components.
 

\subsection{Architecture implementation}


    The first part of the architecture is the Python part, that handles all the 
    communication with the servers. It first opens a TCP/IP connection with the 
    MASSim server, and authenticates. Then it does the same with our Percept 
    Server, that has to be running. It waits for all the agents, and then 
    everything is set to start the simulation.

\subsubsection{Percept-act loop.}

    When the first action-request message is received, this loop starts. It 
    first processes the perception, and then sends it to the Percept Server. 
    This waits for all the other agents' perceptions, merges them, and then sends 
    back a new version that contains everything that has been seen by the agent 
    teammates, but not by itself. Finally, this global perception is asserted 
    into Prolog. 
    As it will be explained below, this part of the program constitutes the 
    decision making module. Therefore, the Python part will query it, for the
    next action to be performed by the agent.
    
\subsubsection{Action requested.}

    As seen in the Figure \ref{fig:architecture}, after the Python part, comes 
    all the processing in Prolog. The perception is already in the knowledge 
    base, that is formed by the ``static'' and ``dynamic'' data. 

    If the agent already has an intention stored, the \textit{cut condition}
    checks whether it makes sense to keep trying to fulfill it. It is a series
    of simple conditions that review the state of the world.

    Then, if there is not any commited intention, or the cut condition decides 
    it is not interesting to keep it, the \textit{beliefs setting process} is 
    started. It generates the possible desires for this step, according to what 
    is stored in the knowledge base, and, for each one of them, the beliefs 
    needed. 
    The decision-making module is implemented in DeLP\cite{Rotstein:2007}
    \cite{Ferretti:2008},
    a defeasible logic 
    programming language that uses argumentation 
    \cite{DBLP:conf/comma/2008}\ to reason with conflicting
    information. 
    Given the set of possible 
    desires and beliefs set by the previous module, it selects the best desire, 
    returning it as the intention that the agent commits to achieve.

    All the plans for all the desires were previously calculated and stored as 
    beliefs, since the amount of steps that they take is used by the 
    argumentation module. The \textit{planning} module selects the one 
    corresponding to the selected intention, and stores it. Then, the 
    execution module only gets the plan, and returns to Python the first 
    action in it.

    However, if the process flow comes from the other branch of Fig. 
    \ref{fig:architecture} (that is, after the cut condition, the agent has an 
    intention), the execution is not that simple. Since skipping the decision-
    taking makes this branch insignificant in terms of time, we decided to 
    recalculate the plan. This might help us when a better path is discovered, 
    even though this is unlikely.

    Back again in Python, with the returned action, an XML is formed to be sent 
    to the server.

\subsection{Programming languages, platforms and tools}

    The agent system was implemented using Python 2.7 and SWI Prolog 5.10.5. DeLP 
    \cite{Garcia:2004a}, a defeasible logic programming language, was used as a 
    service within Prolog. 
    Language integration was achieved using the \textit{pyswip}\ library, 
    calling Prolog predicates from Python. The implementation of Defeasible 
    Logic Programming (DeLP) by the LIDIA was used for the deliberative 
    process. They were well-known at the start of the project, and were chosen 
    for precisely those reasons.
    
    No multi-agent programming languages/platforms/frameworks were used due to 
    previous experience indicating a general lack of flexibility, and a lack of 
    familiarity on behalf of the development team.
    
    Both Linux and the Windows operating system were used as development 
    platforms, since the language runtimes chosen for implementation were 
    portable. Some caveats were encountered however.

    We used \textit{git}\ as our revision control system. In general, we did 
    not spend much time in learning it, since some of us had already worked 
    with it.
    
\subsubsection{DeLP.}
    
    \newcommand{\drule}[2]{\mbox{$ #1\; \defleftarrow \; #2$}}
    \newcommand{\defleftarrow}{{\raise1.5pt\hbox{\tiny\defleft}}}
    \newcommand{\defleft}{\mbox{---\hspace{-1.5pt}\raise.05pt\hbox{$<$}}}

    In DeLP\cite{Garcia:2004a}, knowledge is represented using facts, strict rules
    and defeasible rules. Facts and strict rules are ground literals representing
    firm information that can not be challenged. \textit{Defeasible Rules}
    (d-rules) are denoted $\drule{L_0}{L_1, \ldots, L_n}$ (where $L_i$ are literals)
    and represent tentative information. These rules may be used if nothing could
    be posed against it. A d-rule \textit{``\drule{Head}{Body}''} expresses that
    \textit{``reasons to believe in Body give reasons to believe in Head''}. A DeLP
    program is a set of facts, strict rules and defeasible rules. 

    {\it Strong negation} is allowed in the head of program rules, and hence, may
    be used to represent contradictory knowledge. From such a program contradictory
    literals could be derived, however,  the set of facts and strict rules must
    possess certain internal coherence (it has to be non-contradictory). 

    To deal with contradictory information, in DeLP, \emph{arguments} for
    conflicting pieces of information are built and then compared to decide which
    one prevails. The prevailing argument is a \emph{warrant} for the information
    that it supports.

    In DeLP, a query $L$ is \emph{warranted} from a program if a \emph{non-defeated}
    argument that supports $L$ exists. %\Arg\ 



\subsubsection{Benefits.}
    
    Python's amenity to rapid application development and 'batteries-included 
    philosophy' facilitated implementing the communication layer to the MASSim 
    server, parsing of peceptions, rapid addition of planned features and bug 
    correction.

    We made use of Prolog's declarative nature to model states of the world, and it 
    also made more straightforward to implement search algorithms like Uniform Cost 
    Search, and Depth First Search. The zone-coloring algorithm was also 
    implemented in Prolog.
    
    DeLP's capability to deal with conflicting pieces of information was also very
    helpful in order to implement the decision-making module.
    
    Initial plans were to distribute agents on several machines. Each agent runs 
    as a separate process, and communicates with others via TCP sockets. After 
    some experience and benchmarking, agents were run on one machine, due to 
    performance issues. 
    Having the choice was a benefit of the proposed design.

\subsubsection{Problems encountered.}

    The most difficult problems were related to optimization. Much of our time was 
    spent in reducing the complexity of our algorithms, and the times they 
    were called.

    For the coloring algorithm, we added several improvements, for both 
    optimization and correctness. In essence, since we only had an incomplete 
    version of the full map in every step, we added the concept of ``fog of war`'' 
    to the agents, assuming always in a pessimistic way. 

    For both search algorithms, the Depth First Search and the Uniform Cost 
    Search, we added conditions that could cut several branches, when they were 
    expanding to unwanted nodes. This conditions were set by the caller, since 
    they depend on the context of the problem.

    For the UCS, we first used a simple stack implemented with a list, to keep 
    track of the frontier, because of Prolog's inability to work with arrays. This 
    would have allowed us to develop a heap data structure, to be used in a 
    priority queue. Lately, we found a Prolog library that implemented this data 
    structure, and the migration was pleasantly straightforward.

    Finally, for this last algorithm too, we added an important optimization 
    that allowed us to call it several times, with the virtual cost of only one 
    call. It was done using memoization, and a more thoughtful invocation.

% TODO LO DE LAS LINEAS DE CODIGO

\section{Strategies, Details, and Statistics}

    In this section, we will explain the main characteristics of the agents' 
    strategies, as well as several implementation details, such as algorithms
    used, and agents' organization.

\subsection{Strategy}

    The main strategy of the team consists of detecting profitable zones from the 
    explored vertices, and positioning the agents correctly to maintain, defend 
    and expand the zones. 
    
    The agents coordinate in an implicit way. This is, the  information shared 
    consists only of the perception received, without having neither 
    preprocessed beliefs, nor control variables. The agents do not communicate
    their intention, or plans, so any coordination that they may have has been
    achieved implicitly.
    % TODO: revisar el inglés de esta oración. No nos convence el "or" de dos
    % lineas antes.
    
\subsubsection{Zone conquering.}
    
    The agents calculate whether they are in a zone, or not. This is achieved
    by checking the color of their node (received in the perception), and if
    a neighbor of it is also colored by its team (if that doesn't happen, the
    node do not increase the zone points).
    
    If an agent is not being part of any zone, it tries to regroup with a 
    partner. 
    When a zone is formed, and the agent is part of it, for each potentially 
    beneficial neighbor node, the agent calculates how much points would they 
    win if it moves.
    This is done with our reimplementation of the coloring algorithm used by
    the MASSim server.
    This information is used by the decision taking module.
    
    Agents make no assumption about the map topology. They will prefer higher 
    valued nodes over lower ones.

    If the expansion intention explained is selected and carried on, then a new 
    better zone is implicitly conquered.

\subsubsection{Attacking and defending.}
    
    Both attacking and defense are implicitly implemented. Sabouteurs attack 
    enemies that are near, so they might attack them if they enter our team's 
    zone, as well as when they are in their zone. Any other agent of another role
    can go to a node that has, for example, two agents, one for each team, in order
    to expand the zone, occupying the contested node, and implicitly defending it.
    
\subsection{Implementation}

    Here are the details of the implementation of the different parts of the
    agents.

\subsubsection{Path planning.}

    Path planning is implemented with an Uniform Cost Search 
    \cite{Russell:2003:AIM:773294}. 
    What we tried to minimize was the amount of steps required to achieve the 
    goal, rather than the spent energy. 
    The returned result is the list of actions to be done, rather than the list of 
    nodes.
    
    Since this algorithm can be called several times in one step, given that the 
    actual amount of steps spent by an intention is taken under consideration by 
    the decision-taking module, it was crucial to perform several optimizations in 
    it. In the end, this allowed us to run all the agents in a single machine, 
    during the competition.
    
    The plans are as long as the selected intention requieres. This may 
    sound excesive, but the possible goals were previously selected for their 
    potential, taking into consideration their distances (in nodes, not in 
    actions). However, plans are recalculated in every step, as explained earlier.


\subsubsection{Buying.}

    Agents follow a list of predefined buying actions, when the necessary amount 
    of money is reached.
    
    Many different approaches were considered, e.g. a more thoughtful one, taking
    into consideration the amount of deaths of our team and of our enemy.
    However, this simple approach was the easiest to implement, and it was
    proven to be the most succesful one.

\subsubsection{Achievements.}
    Achievements are not taken under consideration. However, agents can
    achieve a significant number of them, since this behavior is implicitly 
    implemented.

    % TODO explicar que no se apunta EXPLICITAMENTE a buscar achievements 
    % porque surgen del proceso de razonamiento
    
\subsubsection{Phases.}

    Our agents do not change their behavior during runtime. It is actually very 
    easy to add this feature, but we had not enough time to implement it.
    % TODO explicar lo de las fases
    

\subsubsection{Communication.}

    Some functionality provided by the \textit{eismassim} library was
    reimplemented in a connection library in Python.

    On each perceive/act cycle, agents receive the percept from the MASSim server, 
    separate the information which will remain private and which will be shared. 
    The public part of the percept is sent to the percept server, which performs a 
    union of all percepts and send the difference back to each agent. After 
    receiving the joint percept, the agents enter a belief setting phase, and 
    later an argumentative phase.
    
\subsection{Agents' organization}

    The only organization that the agents have is the proper given by the 
    environment, which is the roles. 
    
    Refering to our actual programming, all the agents have a strong core of common
    code, which is:
    
    \begin{itemize}
        \item all the Python part, that servers as a receive-percepts/send-action client 
        of the server,
        
        \item the Percept Server,
        
        \item an important part of the Prolog code, including all the utilities used, the
        implementation of the BDI architecture, the structure of the 
        decision-taking module, and a considerable part of the arguments used, that
        are common to all the roles.
    \end{itemize}
    
    Apart from all this, each role has a couple of separate files, that have 
    specific code, including the arguments used in the decision-taking module, and
    the setting of the beliefs needed for those arguments. Here is where the 
    individual behavior is set, since the specific actions that can be done by each
    role are being had in consideration here.
    
    Specific values for the decision-taking module for each role are also included
    in these files, and this is what can make that agents of different role act 
    differently, faced to the same situation, according to our judgment.
    
    This separation is negligible, having in consideration the amount of code 
    written for the agents, and may be near a 5\% of it. However, this has been proven to
    be more than enough to modify considerably the behavior of the agents, thanks to
    the non-monotonic nature of DeLP, the argumentation language used in the 
    decision-taking module.

\section{Conclusion}

\subsection{Our team, and its development}
    
    Our team is formed by a group of friends, so a strong point in the developing 
    process has been the cohesion and comradeship. We had also worked together,
    in university's projects as well as outside them, in small freelance 
    projects, so we well knew what to expect from each other, and each member's 
    capabilities. 

    Several events in the developing process depended on this bond, i.e. the 
    deployment. We could not connect to the server in the test matches from our 
    university, because of several network problems. Only one day before 
    the competition, we decided to connect everything from one of our partner's house, 
    who had the best infrastructure (connection to Internet, and space in his 
    apartment). Given the fact that we had to spend all night working and testing
    there, for a week the headquarters for our team also became our home.

    However, we had many problems. Many of them were related with our lack of 
    professionality, and lack of experience working in a large project. 
    For example, we had several simple but annoying problems, such as the 
    encoding of our source files, or having different versions of the programs 
    we used to program installed in our computers.

    We are really grateful with our decisions involving programming languages, 
    tools and algorithms. Of course, we had many problems, and much time was spent 
    deciding what to choose. Nevertheless, having all the process in 
    consideration, we may had took the right calls. We should thank the great 
    education given by our university (Universidad Nacional del Sur), and a 
    great mentoring from our professors.

    Many things should and will be improved for next year competence. We have
    gained an important amount of knowledge, capability and experience, that shall 
    help us in all aspects of the developing process, not to mention the 
    important amount of code already written, and all the good decisions already 
    made.

    In particular, we now know the importance of all the testing actually needed 
    for this sort of competition, not only in a virtual environment, as we did, 
    but also in the real context of the contest. There were several hotfixes that 
    were written and deployed at the same time we were facing our competitors, in 
    all but the last day of competition, in which we already had everything in 
    consideration and control.

\subsection{Our thoughts about possible optimizations to the contest}
    
    Many optimizations ocurred to us for the scenario, and the contest in general. 
    For example, more information for the nodes, including something useful for a 
    directed search (i.e., absolute coordinates), would help in the implementation 
    of an A* search, that would decrease the execution times.

    Strategically, the early dominion of the center area played an important part 
    of a good candidate to win a match. It would be useful to try other 
    variations, such as making the borders more important, or others shapes of the 
    map, such as stretched, in form of V, X, O, etc. This would benefit teams that 
    explicitly and thoughtfully look for and conquer good zones, rather than 
    benefiting teams that assume that only one good zone exists, and it's in the 
    middle of the map.

    More informing feedback from the server would be appreciated, specially
    involving errors. This is important for a better and quicker detection of bugs
    involving the communication, i.e. problems with the connection, files sent.

    Finally, we think it will be really helpful for all that we had test matches 
    in a more early stage, in order to have more time to correct errors in the 
    client. Many of us reimplemented the eismassim module, so we are vulnerable to 
    many errors that were difficult to foresee. Early testing would help with 
    that, and detecting infrastructure issues, such as network problems.
    
    
    % TODO pregunta 6 que otros campos
    % Game AI, Robotics

%BIBTEX
    
\bibliographystyle{plainnat} 
\bibliography{bib} 

%\include{questions}

\end{document} 


%referencia del concurso: YATÁ
%referencias estandar para argumentacion y delp: YATÁ
%referencia de XP: NOTÁ
%


%PRELIMINARIES
%
%   no va la explicacion
%
%SYSTEM ANALYSIS AND DESIGN
%
%    proactiveness
%
%    What is the communication strategy and how complex is it?
%
%    Explicar mejor la arqui del Percept server, su mecanismo
%    que cada agente tiene su propia KB
%    que no hay variables compartidos
%
%    Horas   
%
%    abril
%        144 = 6 x 4 x 6
%    mayo
%        144 = 6 x 4 x 6
%    junio
%        144 = 6 x 4 x 6
%        120 = 6 x 20
%    julio
%        144 = 6 x 4 x 6
%        120 = 6 x 20
%    agosto
%        144 = 6 x 4 x 6
%    septiembre
%        360 = 5 x 12 x 6
%
%    1500 total
%
%    % 28 weeks, 8 hs per week, 6 developers ~= 1500
%
%SW ARCHITECTURE
%
%    separar el grafico en dos partes
%    alto nivel, separacion python prolog delp
%    bajo nivel, con todos los modulos e interacciones
%    
%    IÑAKI:
%        como se representa el conocimiento?
%        how does de static dynamic and initial data look like?
%        como se representa entre el agente y el PS?
%        ejemplos de codigo 
%        como se traduce a prolog para meterlo en la KB?
%        UNA BUENA EXPLICACION DEL PIPELINE DE DATOS
%
%    MANU: how does delp look like?  ejemplos de delp
%    MANU: how does an intention look like?
%
%    3.2: Did you use multi-agent programming languages? Why or why not to use a multi-agent programming language?
%
%    3.4: 4. Which development platforms and tools are used? How much time did you invest in learning those?
%            editor for the design process? 
%            what IDE did you use?
%            what abpout debugger for delp?
%            hours spent learning those?
%
%        poner que no usamos IDEs 
%        poner lo del visualizador del arboles dialecticos de delp?
%
%    3.10 To which extend is the reasoning of your agents synchronized with the receive-percepts/send-action cycle?
%
%    3.12 How many lines of code did you write for your software?
%
%
%    cada turno la informacion en la kb se actualiza, se usa el parametro de step para ver la edad y validez de la info
%    ejemplo el algoritmo de coloreo permite generacion de creencias, por lo cual en la kb no esta solamente la percepcion
%
%4 STRATEGIES DETAILS STATISTICS
%
%deberia haber una pequeña introduccion a la seccion en general, entre el titulo de la seccion y el subtitulo Strategy: YATÁ
%
%% TODO explicar lo de las fases: YATÁ
