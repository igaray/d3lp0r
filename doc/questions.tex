Experience from a previous instance of the MAPC was shared with our
teams by members of the ARGONAUTS team from TU Dortmund\cite{Holzgen:2011}. Although the
initial plan was to run tests against other agent teams prior to the
competition, time constraints made this impossible.

\subsection{Software Architecture}
% == Software Architecture 
\setcounter{question}{0}
\begin{question}
Which programming language did you use to
implement the multi-agent system?  
\end{question}

The agent system was implemented using
Python 2.7 and SWI Prolog 5.10.5. DeLP, a defeasible logic language, was used
as a service within Prolog.

\begin{question}
Did you use multi-agent programming languages? Why or why not to use a
multi-agent programming language?  
\end{question}

No multi-agent programming
languages/platforms/frameworks were used. Being the first time we participated
in the contest, we decided not to use technologies that we had absolutely no experience on. Besides,
one of our goals was to develop our own platform in order to keep developing in the future.

\begin{question}
How have you mapped the designed architecture (both multi-agent and
individual agent architectures) to programming codes i.e., how did you
implement specific agent-oriented concepts and designed artifacts using the
programming language?  
\end{question}

The perception is processed by the Python program, that
parses the XML. Then, it sends it to the Percept Server that every step merges
all perceptions, and delivers them back to the agents.  The Python code
asserts all the perception into Prolog, then querying it for the next action
to be executed.  Prolog handles all the decision making, argumentation and
planning, and returns the action binded to a variable to Python, that then
generates with it an XML to be sent to the server.

\begin{question}
Which development platforms and tools are used? How much time did you
invest in learning those?  
\end{question}

All our code was written using either vim (on Linux) or Notepad++ (on Windows). We used no IDEs, but
occasionally we did use the SWI-Prolog integrated debugger.

\begin{question}
Which runtime platforms and tools (e.g. Jade, AgentScape, simply Java,
....) are used? 
\end{question}

How much time did you invest in learning those?  Python and
Prolog were the chosen languages for the development of the system. Most of us
had already worked with both of them, so we did not spend much time learning
those.

\begin{question}
What features were missing in your language choice that would have
facilitated your development task?
\end{question}



\begin{question}
What features of your programming language has simplified your development
task?  
\end{question}

Python's amenity to rapid application development and
'batteries-included philosophy' facilitated implementing the communication
layer to the MASSim server, parsing of peceptions, rapid addition of planned
features and bug correction.  We made use of Prolog's declarative nature to
model states of the world, and it also made it more straightforward to
implement search algorithms.

\begin{question}
Which algorithms are used/implemented?  
\end{question}

Search algorithms, as Uniform Cost
Search and Depth First Search, as well as the zone-coloring algorithm were
implemented in Prolog.  The implementation of Defeasible Logic Programming
(DeLP) by the LIDIA was used for the deliberative process.

\begin{question}
How did you distribute the agents on several machines? And if you did not
please justify why.  
\end{question}

Initial plans were to distribute agents on several
machines. Each agents runs as a separate process, and communicates with others
via TCP sockets. After some experience and benchmarking, agents were run on one
machine due to performance issues. Having the choice was a benefit of the
proposed design.

\begin{question}
To which extend is the reasoning of your agents synchronized with the
receive-percepts/send-action cycle?
\end{question}

All the reasoning is done after receiving the percepts, and before sending the
action.


\begin{question}
What part of the development was most difficult/complex? What kind of
problems have you found and how are they solved?  
\end{question}

The most difficult problems
were related to optimization. Much of our time has been spent in reducing the
complexity of our algorithms, and the times they are called.

\begin{question}
How many lines of code did you write for your software?  
\end{question}

Total LOC is 5842.

% == Strategies, Details, and Statistics
\subsection{Strategies, Details, and Statistics}
\setcounter{question}{0}
\begin{question}
What is the main strategy of your team?
\end{question}

The main strategy of the team consists of detecting profitable zones
from the explored vertices, and positioning the agents correctly to maintain,
defend and expand the zones.

\begin{question}
How does the overall team work together? (coordination, information
sharing, ...) 
\end{question}

The agents coordinate in an implicit way. This is, the  information shared 
consists only of the perception received, without having neither 
preprocessed beliefs, nor control variables. The agents do not communicate
their intention, or plans, so any coordination that they may have has been
achieved implicitly.

\begin{question}
How do your agents analyze the topology of the map? And how do they exploit
their findings? 
\end{question}

Agents make no assumption about the map topology. They will
prefer higher valued nodes over lower ones.


\begin{question}
How do your agents communicate with the server?  
\end{question}

Some functionality
provided by the eismassim library was reimplemented in a connection library in
Python.

\begin{question}
How do you implement the roles of the agents? Which strategies do the
different roles implement?  
\end{question}

Agents recover their assigned role from the
simulation start message.  

\begin{question}
How do you find good zones? How do you estimate the value of zones?  
\end{question}

If an
agent is not being part of any zone, it tries to regroup with a partner.  When
a zone is formed, and the agent is part of it, for each potentally beneficial
neighbor node, the agent calculates how much points would they win if it
moves, and that information is used by the decision taking module.

\begin{question}
How do you conquer zones? How do you defend zones if attacked? Do you
attack zones?  
\end{question}

Both attacking and defense are implicitly implemented.
Sabouteurs attack enemies that are near, so they might attack them if they
enter our team's zone, as well as when they are in their zone. Any other agent
of another role can go to a node that has, for example, two agents, one for
each team, in order to expand the zone, occupying the contested node, and
implicitly defending it.

\begin{question}
Can your agents change their behavior during runtime? If so, what triggers
the changes?  
\end{question}

Our agents do not change their behavior during runtime. It is
actually very easy to add this feature, but we had not enough time to
implement this.

\begin{question}
What algorithm(s) do you use for agent path planning?  
\end{question}

Path planning is
implemented with an Uniform Cost Search. What we tried to minimize was the
amount of steps required to achieve the goal, rather than the spent energy.
The returned result is the list of actions to be done.

\begin{question}
How do you make use of the buying-mechanism?  
\end{question}

Agents followed a list of
predefined buying actions, when the necessary amount of money was reached.

\begin{question}
How important are achievements for your overall strategy?  
\end{question}

Our agents did
not have achievements in consideration. However, they managed to achieve a
significant number of them, since this behavior was implicitly implemented.

\begin{question}
Do your agents have an explicit mental state?
\end{question}


\begin{question}
How do your agents communicate? And what do they communicate?  
\end{question}

Agents only
communicate their perceptions via a perception server implemented in Python.

On each perceive/act cycle, agents receive the percept from the
MASSim server, separate the information which will remain private and which
will be shared.  The public part of the percept is sent to the percept server,
which performs a union of all percept and send the difference back to each
agent. After receiving the joint percept, the agents enter a belief setting
phase, and later an argumentation phase.


\begin{question}
How do you organize your agents? Do you use e.g. hierarchies? Is your
organization implicit or explicit?
\end{question}
There is no agent hierarchy, and since the decision-making process takes
place individually for each agent, there is no organization between them.


\begin{question}
Is most of your agents behavior emergent on an individual and team
level?
\end{question}
Since each agent makes decisions considering the other agents' status,
team behavior is emergent from individual behavior.

\begin{question}
If your agents perform some planning, how many steps do they plan ahead?
\end{question}

The agents make plans as long as the selected intention requieres. This may
sound excesive, but the possible goals were previously selected for their
potential, taking in consideration their distance (in nodes, not in actions).
However, plans are recalculated in every step.

% == Conclusion
\subsection{Conclusion}
\setcounter{question}{0}
\begin{question}
What have you learned from the participation in the contest?
\end{question}

Being our first experience building a system this size, we learned several 
lessons about working in big projects, such as setting standards and 
synchronizing versions of the technologies used.

\begin{question}
Which are the strong and weak points of the team?
\end{question}

    Our team is formed by a group of friends, so a strong point in the
    developing proccess was the cohesion and comradeship. We had
    already worked together, both in university's projects and in
    small freelance projects, so we well knew what to expect from each other,
    and each member's capabilities.

\begin{question}  
How suitable was the chosen programming language, methodology, tools, and
algorithms?
\end{question}

    We are really happy with our decisions involving programming languages,
    tools and algorithms. Of course, we had many problems, and much time was
    spent deciding what to choose, but having all the proccess in
    consideration, we may had took the right calls.

\begin{question}
What can be improved in the context for next year?
\end{question}

There were several hotfixes that were written and deployed at the same 
time we were facing our competitors due to the lack of testing in the 
actual context of the competition. This situation should obviously not 
happen, and adding much more real testing is one of our main priorities 
for next year's competition.

\begin{question}
Why did your team perform as it did? Why did the other teams perform
better/worse than you did?
\end{question}
We had several problems that didn't let us perform as good as we expected.
Our lack of experience in this kind of contests, unexpected network and latency
problems, as well as some bugs that caused critical performance issues, caused
our team to lose several matches that could've been won otherwise.

\begin{question}
Which other research fields might be interested in the Multi-Agent
Programming Contest?
\end{question}
Both Robotics and Gaming AI are interesting fields that could benefit from
participating in the contest.

\begin{question}
How can the current scenario be optimized? How would those optimization pay
off?
\end{question}

    More information for the nodes, including something useful for a directed
    search (i.e., absolute coordinates), would help in
    the implementation of a A* search (which would decrease execution time).
    Defining the most valuable zones randomly would benefit teams that
    thoughtfully look for and conquer good zones, rather than teams that assume
    that the center of the map is the most valuable zone and don't explore the
    rest.

    A more informing/informational/apprising? feedback from the server would
    be appreciated, specially involving errors.

    Finally, we think it will be really helpful for all that we had test
    matches in a more early stage, in order to have more time to correct
    errors in the client.
